{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from lib.utils import set_chatgpt_env, set_langsmith_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_langsmith_env()\n",
    "set_langsmith_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema using Pydantic (similar to Zod)\n",
    "class GmtTimeSchema(BaseModel):\n",
    "    city: str\n",
    "\n",
    "# Define the tool function\n",
    "@tool\n",
    "def gmt_time_tool(city: str) -> str:\n",
    "    \"\"\"Check local time in a specified city.\n",
    "    The API is randomly available every third call.\"\"\"\n",
    "    \n",
    "    service_is_working = random.randint(0, 2)  # Randomly fails every third call\n",
    "    if service_is_working != 2:\n",
    "        return f\"The local time in {city} is 6:30pm.\"\n",
    "    else:\n",
    "        return \"Error 404\"\n",
    "    \n",
    "def call_model_node(state):\n",
    "    \"\"\"Simulates an AI model invoking tools.\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    llm_with_tools = llm.bind_tools([gmt_time_tool])  # Simulated tool binding\n",
    "    result = llm_with_tools.invoke(messages)  # Simulate tool invocation\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "# Helper function to get the last message\n",
    "def get_last_message(state):\n",
    "    \"\"\"Returns the last message from the state.\"\"\"\n",
    "    return state.get(\"messages\", [])[-1] if state.get(\"messages\") else {}\n",
    "\n",
    "# Define function to decide whether to continue or end\n",
    "def should_continue(state):\n",
    "    \"\"\"Decides whether to continue processing or end.\"\"\"\n",
    "    last_message = get_last_message(state)\n",
    "    did_ai_called_any_tools = bool(last_message.get(\"tool_calls\"))\n",
    "    return \"tools\" if did_ai_called_any_tools else END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode([gmt_time_tool])\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"agent\", call_model_node)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
